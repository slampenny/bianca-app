name: Deploy to Staging

on:
  push:
    branches:
      - staging
  workflow_dispatch: # Allow manual triggers

env:
  AWS_REGION: us-east-2
  ECR_REGISTRY: 730335291008.dkr.ecr.us-east-2.amazonaws.com

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write  # Required for OIDC
      actions: read  # Required to checkout other repos
    
    steps:
      - name: Checkout backend code
        uses: actions/checkout@v4
        with:
          path: backend

      - name: Checkout frontend code
        uses: actions/checkout@v4
        with:
          repository: slampenny/bianca-app-frontend
          ref: staging  # Use staging branch for staging deployments
          path: frontend
          token: ${{ secrets.FRONTEND_REPO_TOKEN || secrets.GITHUB_TOKEN }}
        continue-on-error: true
        id: checkout-frontend
        # Note: Uses FRONTEND_REPO_TOKEN if available, otherwise GITHUB_TOKEN

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-Staging-Deploy

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push images in parallel
        run: |
          # Build all images in parallel using background jobs
          echo "ğŸ”¨ Building images in parallel..."
          
          cd backend
          docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-backend:staging . &
          BACKEND_PID=$!
          
          # Only build frontend if checkout succeeded
          if [ "${{ steps.checkout-frontend.outcome }}" == "success" ] && [ -d "../frontend" ]; then
            cd ../frontend
            docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-frontend:staging \
              -f devops/Dockerfile --build-arg BUILD_ENV=staging . &
            FRONTEND_PID=$!
            BUILD_FRONTEND=true
          else
            echo "âš ï¸  Frontend checkout failed or not available, skipping frontend build"
            BUILD_FRONTEND=false
          fi
          
          cd ../backend/devops/asterisk
          docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-asterisk:staging . &
          ASTERISK_PID=$!
          
          # Wait for all builds
          wait $BACKEND_PID && echo "âœ… Backend build complete" || exit 1
          if [ "$BUILD_FRONTEND" = true ]; then
            wait $FRONTEND_PID && echo "âœ… Frontend build complete" || exit 1
          fi
          wait $ASTERISK_PID && echo "âœ… Asterisk build complete" || exit 1
          
          # Push all images in parallel
          echo "ğŸ“¦ Pushing images to ECR in parallel..."
          docker push ${{ env.ECR_REGISTRY }}/bianca-app-backend:staging &
          if [ "$BUILD_FRONTEND" = true ]; then
            docker push ${{ env.ECR_REGISTRY }}/bianca-app-frontend:staging &
          fi
          docker push ${{ env.ECR_REGISTRY }}/bianca-app-asterisk:staging &
          
          wait
          echo "âœ… All images pushed successfully!"

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Deploy infrastructure (if changed)
        working-directory: backend/devops/terraform
        run: |
          terraform init
          
          echo "ğŸ“‹ Running terraform plan..."
          set +e  # Don't exit on error immediately
          PLAN_OUTPUT=$(terraform plan -no-color -input=false 2>&1)
          PLAN_EXIT=$?
          echo "$PLAN_OUTPUT"
          set -e  # Re-enable exit on error
          
          if [ $PLAN_EXIT -ne 0 ]; then
            echo "âŒ Terraform plan failed with exit code $PLAN_EXIT"
            echo "This might be due to missing permissions or configuration issues."
            echo "Skipping Terraform apply for now - infrastructure may need manual attention."
            # Don't fail the entire workflow - containers can still be updated
            exit 0
          elif echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "âœ… No infrastructure changes detected. Skipping Terraform apply."
          else
            echo "ğŸ“‹ Infrastructure changes detected. Applying..."
            terraform apply -auto-approve -no-color
            echo "âœ… Terraform apply completed"
          fi

      - name: Setup SSH key
        run: |
          # Store SSH private key from GitHub Secrets
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > /tmp/staging-key.pem
          chmod 600 /tmp/staging-key.pem
          echo "âœ… SSH key configured"

      - name: Update staging containers
        run: |
          # Get staging instance ID (check for running instances first)
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=bianca-staging" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text --region ${{ env.AWS_REGION }})
          
          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
            echo "âš ï¸  Staging instance not running. Checking if it exists..."
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=bianca-staging" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text --region ${{ env.AWS_REGION }})
            
            if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
              echo "âŒ Staging instance not found"
              exit 1
            fi
            
            echo "ğŸ”„ Starting stopped instance..."
            aws ec2 start-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "â³ Waiting for instance to start..."
            aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "âœ… Instance started, waiting for SSH service to be ready..."
            sleep 30  # Give instance time to initialize SSH service
            INSTANCE_JUST_STARTED=true
          else
            INSTANCE_JUST_STARTED=false
          fi
          
          echo "ğŸ“¡ Updating containers on instance: $INSTANCE_ID"
          
          # Get instance public IP for SSH
          echo "â³ Getting instance public IP..."
          INSTANCE_IP=$(aws ec2 describe-instances \
            --instance-ids "$INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)
          
          if [ -z "$INSTANCE_IP" ] || [ "$INSTANCE_IP" == "None" ]; then
            echo "âŒ Could not get instance public IP"
            exit 1
          fi
          
          echo "ğŸ“¡ Connecting to instance: $INSTANCE_IP"
          
          # Wait for SSH to be ready (longer wait if instance was just started)
          if [ "$INSTANCE_JUST_STARTED" = "true" ]; then
            MAX_ATTEMPTS=24  # 2 minutes for newly started instances
            echo "â³ Waiting for SSH to be ready (instance was just started, allowing up to 2 minutes)..."
          else
            MAX_ATTEMPTS=12  # 1 minute for already running instances
            echo "â³ Waiting for SSH to be ready..."
          fi
          
          SSH_READY=false
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes -i /tmp/staging-key.pem ec2-user@$INSTANCE_IP "echo 'SSH ready'" 2>/dev/null; then
              echo "âœ… SSH is ready"
              SSH_READY=true
              break
            fi
            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "âš ï¸  SSH not ready after $((MAX_ATTEMPTS * 5)) seconds"
            else
              echo "   Waiting for SSH... ($i/$MAX_ATTEMPTS)"
              sleep 5
            fi
          done
          
          if [ "$SSH_READY" != "true" ]; then
            echo "âš ï¸  SSH connection failed. Trying SSM as fallback..."
            
            # Check if SSM is available
            SSM_READY=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query 'InstanceInformationList[0].PingStatus' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Inactive")
            
            if [ "$SSM_READY" = "Online" ]; then
              echo "âœ… SSM is available, using SSM instead of SSH"
              USE_SSM=true
            else
              echo "âŒ Both SSH and SSM are unavailable. Cannot proceed with deployment."
              echo "   Instance IP: $INSTANCE_IP"
              echo "   Instance ID: $INSTANCE_ID"
              echo "   SSM Status: $SSM_READY"
              echo "   The instance may need to be restarted or checked manually."
              exit 1
            fi
          else
            USE_SSM=false
          fi
          
          if [ "$USE_SSM" = "true" ]; then
            # Use SSM to update containers
            echo "ğŸ“¡ Executing deployment via SSM..."
            aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=[
                "cd /opt/bianca-staging",
                "ECR_TOKEN_FILE=/tmp/ecr-token-$(date +%Y%m%d)",
                "if [ ! -f \"$ECR_TOKEN_FILE\" ]; then aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 730335291008.dkr.ecr.us-east-2.amazonaws.com && touch \"$ECR_TOKEN_FILE\"; else echo \"Using cached ECR token\"; fi",
                "echo \"ğŸ“¥ Pulling latest images...\"",
                "docker-compose pull",
                "echo \"ğŸ”„ Stopping old containers...\"",
                "docker-compose down 2>/dev/null || true",
                "echo \"ğŸš€ Starting containers...\"",
                "docker-compose up -d --remove-orphans",
                "sleep 5",
                "docker ps --format \"table {{.Names}}\t{{.Image}}\t{{.Status}}\" | grep staging_ || true"
              ]' \
              --region ${{ env.AWS_REGION }} \
              --output text \
              --query 'Command.CommandId' > /tmp/ssm-command-id.txt
            
            COMMAND_ID=$(cat /tmp/ssm-command-id.txt)
            echo "â³ Waiting for SSM command to complete..."
            
            # Wait for command to complete (up to 5 minutes)
            for i in {1..60}; do
              STATUS=$(aws ssm get-command-invocation \
                --command-id "$COMMAND_ID" \
                --instance-id "$INSTANCE_ID" \
                --query 'Status' \
                --output text \
                --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Unknown")
              
              if [ "$STATUS" = "Success" ]; then
                echo "âœ… SSM command completed successfully"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardOutputContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }}
                break
              elif [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] || [ "$STATUS" = "TimedOut" ]; then
                echo "âŒ SSM command failed with status: $STATUS"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardErrorContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }} || true
                exit 1
              fi
              
              if [ $i -eq 60 ]; then
                echo "âŒ SSM command timed out after 5 minutes"
                exit 1
              fi
              
              sleep 5
            done
          else
            # Use SSH to update containers
            ssh -o StrictHostKeyChecking=no -i /tmp/staging-key.pem ec2-user@$INSTANCE_IP bash -c "
            set -e
            cd /opt/bianca-staging &&
            echo 'ğŸ“¦ Logging into ECR...' &&
            ECR_TOKEN_FILE=/tmp/ecr-token-\$(date +%Y%m%d) &&
            if [ ! -f \"\$ECR_TOKEN_FILE\" ]; then
              aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin 730335291008.dkr.ecr.us-east-2.amazonaws.com &&
              touch \"\$ECR_TOKEN_FILE\"
            else
              echo '   Using cached ECR token'
            fi &&
            echo 'ğŸ“¥ Pulling latest images (forcing pull)...' &&
            docker-compose pull 2>&1 | tee /tmp/pull-output.log &&
            echo '' &&
            echo 'ğŸ“Š Pull summary:' &&
            grep -E 'Pulling|Downloaded|Image is up to date|Digest|Status' /tmp/pull-output.log | tail -10 || true &&
            rm -f /tmp/pull-output.log &&
            echo '' &&
            echo 'ğŸ”„ Stopping and removing old containers...' &&
            docker-compose down 2>/dev/null || true &&
            echo 'ğŸš€ Starting containers with new images...' &&
            docker-compose up -d --remove-orphans &&
            echo 'â³ Waiting for containers to start...' &&
            sleep 5 &&
            echo 'ğŸ“‹ Container status:' &&
            docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}' | grep staging_ || true &&
            echo '' &&
            echo 'ğŸ” Checking image versions:' &&
            docker images --format 'table {{.Repository}}\t{{.Tag}}\t{{.CreatedAt}}' | grep 'bianca-app\|ECR' | head -5 || true
          "
          
          # SSH command executes synchronously, so if we get here, it succeeded
          if [ $? -eq 0 ]; then
            echo "âœ… Deployment completed successfully!"
          else
            echo "âŒ Deployment failed"
            exit 1
          fi

