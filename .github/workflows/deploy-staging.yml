name: Deploy to Staging

on:
  push:
    branches:
      - staging
  workflow_dispatch: # Allow manual triggers
    inputs:
      branch:
        description: 'Branch to deploy (defaults to staging)'
        required: false
        default: 'staging'
        type: string

env:
  AWS_REGION: us-east-2
  ECR_REGISTRY: 730335291008.dkr.ecr.us-east-2.amazonaws.com

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write  # Required for OIDC
      actions: read  # Required to checkout other repos
    
    steps:
      - name: Checkout backend code
        uses: actions/checkout@v4
        with:
          path: backend
          ref: ${{ github.event.inputs.branch || github.ref }}

      - name: Checkout frontend code
        uses: actions/checkout@v4
        with:
          repository: slampenny/bianca-app-frontend
          ref: staging  # Use staging branch for staging deployments
          path: frontend
          token: ${{ secrets.FRONTEND_REPO_TOKEN || secrets.GITHUB_TOKEN }}
        continue-on-error: true
        id: checkout-frontend
        # Note: Uses FRONTEND_REPO_TOKEN if available, otherwise GITHUB_TOKEN

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-Staging-Deploy

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Build and push images in parallel
        run: |
          # Build all images in parallel using background jobs
          echo "ğŸ”¨ Building images in parallel..."
          
          cd backend
          docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-backend:staging . &
          BACKEND_PID=$!
          
          # Only build frontend if checkout succeeded
          if [ "${{ steps.checkout-frontend.outcome }}" == "success" ] && [ -d "../frontend" ]; then
            cd ../frontend
            docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-frontend:staging \
              -f devops/Dockerfile --build-arg BUILD_ENV=staging . &
            FRONTEND_PID=$!
            BUILD_FRONTEND=true
          else
            echo "âš ï¸  Frontend checkout failed or not available, skipping frontend build"
            BUILD_FRONTEND=false
          fi
          
          cd ../backend/devops/asterisk
          docker build -t ${{ env.ECR_REGISTRY }}/bianca-app-asterisk:staging . &
          ASTERISK_PID=$!
          
          # Wait for all builds
          wait $BACKEND_PID && echo "âœ… Backend build complete" || exit 1
          if [ "$BUILD_FRONTEND" = true ]; then
            wait $FRONTEND_PID && echo "âœ… Frontend build complete" || exit 1
          fi
          wait $ASTERISK_PID && echo "âœ… Asterisk build complete" || exit 1
          
          # Push all images in parallel
          echo "ğŸ“¦ Pushing images to ECR in parallel..."
          docker push ${{ env.ECR_REGISTRY }}/bianca-app-backend:staging &
          if [ "$BUILD_FRONTEND" = true ]; then
            docker push ${{ env.ECR_REGISTRY }}/bianca-app-frontend:staging &
          fi
          docker push ${{ env.ECR_REGISTRY }}/bianca-app-asterisk:staging &
          
          wait
          echo "âœ… All images pushed successfully!"

      - name: Install Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Deploy infrastructure (if changed)
        working-directory: backend/devops/terraform
        run: |
          echo "ğŸ”§ Initializing Terraform..."
          set +e  # Don't exit on error immediately
          terraform init
          INIT_EXIT=$?
          set -e  # Re-enable exit on error
          
          if [ $INIT_EXIT -ne 0 ]; then
            echo "âš ï¸  Terraform init failed with exit code $INIT_EXIT"
            echo "This might be due to state lock or backend configuration issues."
            echo "Skipping Terraform operations - infrastructure may need manual attention."
            echo "Continuing with container deployment..."
            exit 0
          fi
          
          echo "ğŸ“‹ Running terraform plan..."
          set +e  # Don't exit on error immediately
          PLAN_OUTPUT=$(terraform plan -no-color -input=false 2>&1)
          PLAN_EXIT=$?
          echo "$PLAN_OUTPUT"
          set -e  # Re-enable exit on error
          
          if [ $PLAN_EXIT -ne 0 ]; then
            echo "âŒ Terraform plan failed with exit code $PLAN_EXIT"
            echo "This might be due to missing permissions or configuration issues."
            echo "Skipping Terraform apply for now - infrastructure may need manual attention."
            # Don't fail the entire workflow - containers can still be updated
            exit 0
          elif echo "$PLAN_OUTPUT" | grep -q "No changes"; then
            echo "âœ… No infrastructure changes detected. Skipping Terraform apply."
          else
            echo "ğŸ“‹ Infrastructure changes detected. Applying..."
            set +e
            terraform apply -auto-approve -no-color
            APPLY_EXIT=$?
            set -e
            if [ $APPLY_EXIT -eq 0 ]; then
              echo "âœ… Terraform apply completed"
            else
              echo "âš ï¸  Terraform apply failed with exit code $APPLY_EXIT"
              echo "Continuing with container deployment..."
            fi
          fi

      - name: Setup SSH key
        run: |
          # Store SSH private key from GitHub Secrets
          echo "${{ secrets.STAGING_SSH_PRIVATE_KEY }}" > /tmp/staging-key.pem
          chmod 600 /tmp/staging-key.pem
          echo "âœ… SSH key configured"

      - name: Deploy to staging using CodeDeploy
        run: |
          echo "ğŸš€ Deploying to staging using AWS CodeDeploy..."
          
          # Get staging instance ID
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=bianca-staging" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text \
            --region ${{ env.AWS_REGION }})
          
          if [ "$INSTANCE_ID" = "None" ] || [ -z "$INSTANCE_ID" ]; then
            echo "âš ï¸  No running staging instance found. Starting instance..."
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=bianca-staging" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text \
              --region ${{ env.AWS_REGION }})
            
            aws ec2 start-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "â³ Waiting for instance to start..."
            aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "â³ Waiting 60 seconds for services to initialize..."
            sleep 60
          fi
          
          # Check and install CodeDeploy agent if needed
          echo "ğŸ” Checking CodeDeploy agent status..."
          SSM_READY=$(aws ssm describe-instance-information \
            --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
            --query 'InstanceInformationList[0].PingStatus' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Inactive")
          
          if [ "$SSM_READY" = "Online" ]; then
            echo "   SSM is available. Checking CodeDeploy agent..."
            AGENT_CHECK_CMD_ID=$(aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=["systemctl is-active codedeploy-agent || echo \"NOT_INSTALLED\""]' \
              --region ${{ env.AWS_REGION }} \
              --output text \
              --query 'Command.CommandId' 2>/dev/null || echo "")
            
            if [ -n "$AGENT_CHECK_CMD_ID" ]; then
              sleep 5
              AGENT_STATUS=$(aws ssm get-command-invocation \
                --command-id "$AGENT_CHECK_CMD_ID" \
                --instance-id "$INSTANCE_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'StandardOutputContent' \
                --output text 2>/dev/null | tr -d '\n' || echo "unknown")
              
              if [ "$AGENT_STATUS" != "active" ] && [ "$AGENT_STATUS" != "inactive" ]; then
                echo "âš ï¸  CodeDeploy agent not installed. Installing now..."
                INSTALL_CMD_ID=$(aws ssm send-command \
                  --instance-ids "$INSTANCE_ID" \
                  --document-name "AWS-RunShellScript" \
                  --parameters 'commands=["sudo yum install -y ruby wget", "cd /tmp", "sudo wget https://aws-codedeploy-us-east-2.s3.us-east-2.amazonaws.com/latest/install", "sudo chmod +x ./install", "sudo ./install auto", "sudo systemctl enable codedeploy-agent", "sudo systemctl start codedeploy-agent", "sleep 5", "sudo systemctl status codedeploy-agent | head -5"]' \
                  --region ${{ env.AWS_REGION }} \
                  --output text \
                  --query 'Command.CommandId' 2>/dev/null || echo "")
                
                if [ -n "$INSTALL_CMD_ID" ]; then
                  echo "   Waiting for agent installation (up to 2 minutes)..."
                  for i in {1..24}; do
                    INSTALL_STATUS=$(aws ssm get-command-invocation \
                      --command-id "$INSTALL_CMD_ID" \
                      --instance-id "$INSTANCE_ID" \
                      --region ${{ env.AWS_REGION }} \
                      --query 'Status' \
                      --output text 2>/dev/null || echo "Unknown")
                    
                    if [ "$INSTALL_STATUS" = "Success" ] || [ "$INSTALL_STATUS" = "Failed" ]; then
                      aws ssm get-command-invocation \
                        --command-id "$INSTALL_CMD_ID" \
                        --instance-id "$INSTANCE_ID" \
                        --region ${{ env.AWS_REGION }} \
                        --query 'StandardOutputContent' \
                        --output text 2>/dev/null | tail -10
                      break
                    fi
                    sleep 5
                  done
                  echo "   Waiting 30 seconds for agent to initialize..."
                  sleep 30
                fi
              else
                echo "âœ… CodeDeploy agent is installed (status: $AGENT_STATUS)"
              fi
            fi
          else
            echo "âš ï¸  SSM is not available. CodeDeploy agent may not be installed."
            echo "   The instance may need to be recreated to get the updated userdata."
            echo "   Continuing anyway - deployment will fail if agent is not installed..."
          fi
          
          echo "ğŸ“¦ Packaging deployment artifacts..."
          
          # Get the workspace root (where GitHub Actions checks out code)
          WORKSPACE_ROOT="$GITHUB_WORKSPACE"
          if [ -z "$WORKSPACE_ROOT" ]; then
            # Fallback: assume we're in the repo root
            WORKSPACE_ROOT=$(pwd)
          fi
          
          echo "   Workspace root: $WORKSPACE_ROOT"
          
          # Create deployment package
          DEPLOY_DIR=$(mktemp -d)
          cd "$DEPLOY_DIR"
          
          # Copy appspec.yml and scripts from the backend directory
          # The backend code is checked out to the 'backend' directory in the workspace
          CODEDEPLOY_SOURCE="$WORKSPACE_ROOT/backend/devops/codedeploy"
          if [ -d "$CODEDEPLOY_SOURCE" ]; then
            echo "   Copying CodeDeploy files from $CODEDEPLOY_SOURCE..."
            cp -r "$CODEDEPLOY_SOURCE"/* .
          else
            echo "âŒ CodeDeploy directory not found at $CODEDEPLOY_SOURCE"
            echo "   Current directory: $(pwd)"
            echo "   Workspace root: $WORKSPACE_ROOT"
            ls -la "$WORKSPACE_ROOT/backend/devops/" 2>/dev/null || echo "backend/devops/ doesn't exist"
            ls -la "$WORKSPACE_ROOT/" 2>/dev/null | head -20
            exit 1
          fi
          
          # Copy docker-compose.yml from staging directory structure
          mkdir -p /opt/bianca-staging
          # We'll use the docker-compose.yml that's already on the instance
          # CodeDeploy will just trigger the deployment hooks
          
          # Create a simple deployment package
          zip -r deployment.zip . -x "*.git*" "*.DS_Store*"
          
          # Upload to S3
          S3_BUCKET="bianca-codedeploy-artifacts-730335291008"
          S3_KEY="staging/deployment-$(date +%Y%m%d-%H%M%S).zip"
          
          echo "ğŸ“¤ Uploading deployment package to S3..."
          aws s3 cp deployment.zip "s3://${S3_BUCKET}/${S3_KEY}" --region ${{ env.AWS_REGION }}
          
          # Check for existing deployments
          echo "ğŸ” Checking for existing deployments..."
          EXISTING_DEPLOYMENT=$(aws deploy list-deployments \
            --application-name bianca-staging \
            --deployment-group-name bianca-staging-ec2 \
            --include-only-statuses Created InProgress Queued InProgress ReadyStop \
            --max-items 1 \
            --region ${{ env.AWS_REGION }} \
            --query 'deployments[0]' \
            --output text 2>/dev/null || echo "None")
          
          if [ "$EXISTING_DEPLOYMENT" != "None" ] && [ -n "$EXISTING_DEPLOYMENT" ]; then
            echo "âš ï¸  Found existing deployment: $EXISTING_DEPLOYMENT"
            EXISTING_STATUS=$(aws deploy get-deployment \
              --deployment-id "$EXISTING_DEPLOYMENT" \
              --region ${{ env.AWS_REGION }} \
              --query 'deploymentInfo.status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "   Status: $EXISTING_STATUS"
            
            if [ "$EXISTING_STATUS" = "InProgress" ] || [ "$EXISTING_STATUS" = "Created" ] || [ "$EXISTING_STATUS" = "Queued" ]; then
              echo "â³ Waiting for existing deployment to complete (up to 10 minutes)..."
              # Wait for existing deployment
              for i in {1..60}; do
                STATUS=$(aws deploy get-deployment \
                  --deployment-id "$EXISTING_DEPLOYMENT" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'deploymentInfo.status' \
                  --output text 2>/dev/null || echo "Unknown")
                
                if [ "$STATUS" = "Succeeded" ]; then
                  echo "âœ… Existing deployment completed successfully!"
                  echo "   Creating new deployment with latest code..."
                  break
                elif [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Stopped" ]; then
                  echo "âš ï¸  Existing deployment failed/stopped. Creating new deployment..."
                  break
                fi
                
                if [ $i -eq 60 ]; then
                  echo "âš ï¸  Existing deployment still in progress after 10 minutes. Stopping it and creating new one..."
                  aws deploy stop-deployment \
                    --deployment-id "$EXISTING_DEPLOYMENT" \
                    --region ${{ env.AWS_REGION }} 2>/dev/null || true
                  sleep 5
                  break
                fi
                
                echo "   Waiting... ($i/60) - Status: $STATUS"
                sleep 10
              done
            elif [ "$EXISTING_STATUS" = "Failed" ] || [ "$EXISTING_STATUS" = "Stopped" ]; then
              echo "   Previous deployment failed/stopped. Creating new deployment..."
            fi
          fi
          
          # Create CodeDeploy deployment
          echo "ğŸš€ Creating CodeDeploy deployment..."
          DEPLOYMENT_ID=$(aws deploy create-deployment \
            --application-name bianca-staging \
            --deployment-group-name bianca-staging-ec2 \
            --s3-location bucket=${S3_BUCKET},key=${S3_KEY},bundleType=zip \
            --region ${{ env.AWS_REGION }} \
            --query 'deploymentId' \
            --output text 2>&1)
          
          # Check if deployment creation failed due to limit
          if echo "$DEPLOYMENT_ID" | grep -q "DeploymentLimitExceededException"; then
            echo "âš ï¸  Deployment limit exceeded. Waiting 30 seconds and retrying..."
            sleep 30
            # Try to stop any stuck deployments
            STUCK_DEPLOYMENTS=$(aws deploy list-deployments \
              --application-name bianca-staging \
              --deployment-group-name bianca-staging-ec2 \
              --include-only-statuses Created InProgress Queued \
              --max-items 5 \
              --region ${{ env.AWS_REGION }} \
              --query 'deployments[*]' \
              --output text 2>/dev/null || echo "")
            
            if [ -n "$STUCK_DEPLOYMENTS" ]; then
              echo "   Stopping stuck deployments..."
              for DEP_ID in $STUCK_DEPLOYMENTS; do
                aws deploy stop-deployment \
                  --deployment-id "$DEP_ID" \
                  --region ${{ env.AWS_REGION }} 2>/dev/null || true
              done
              sleep 10
            fi
            
            # Retry creating deployment
            DEPLOYMENT_ID=$(aws deploy create-deployment \
              --application-name bianca-staging \
              --deployment-group-name bianca-staging-ec2 \
              --s3-location bucket=${S3_BUCKET},key=${S3_KEY},bundleType=zip \
              --region ${{ env.AWS_REGION }} \
              --query 'deploymentId' \
              --output text 2>&1)
          fi
          
          # Check if we got a valid deployment ID
          if echo "$DEPLOYMENT_ID" | grep -q "Exception\|Error"; then
            echo "âŒ Failed to create deployment: $DEPLOYMENT_ID"
            exit 1
          fi
          
          echo "â³ Deployment ID: $DEPLOYMENT_ID"
          echo "â³ Waiting for deployment to complete (up to 15 minutes)..."
          
          # Wait for deployment to complete
          for i in {1..90}; do
            STATUS=$(aws deploy get-deployment \
              --deployment-id "$DEPLOYMENT_ID" \
              --region ${{ env.AWS_REGION }} \
              --query 'deploymentInfo.status' \
              --output text 2>/dev/null || echo "Unknown")
            
            echo "   Status: $STATUS ($i/90)"
            
            if [ "$STATUS" = "Succeeded" ]; then
              echo "âœ… Deployment completed successfully!"
              exit 0
            elif [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Stopped" ]; then
              echo "âŒ Deployment failed with status: $STATUS"
              echo "ğŸ“‹ Deployment details:"
              aws deploy get-deployment \
                --deployment-id "$DEPLOYMENT_ID" \
                --region ${{ env.AWS_REGION }} \
                --query 'deploymentInfo' \
                --output json || true
              exit 1
            fi
            
            sleep 10
          done
          
          echo "âš ï¸  Deployment timed out after 15 minutes"
          echo "ğŸ“‹ Final status:"
          aws deploy get-deployment \
            --deployment-id "$DEPLOYMENT_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'deploymentInfo.status' \
            --output text || true
          exit 1

      - name: Update staging containers (Legacy - will be removed)
        if: false  # Disabled - using CodeDeploy instead
        run: |
          # Get staging instance ID (check for running instances first)
          INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=bianca-staging" "Name=instance-state-name,Values=running" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text --region ${{ env.AWS_REGION }})
          
          if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
            echo "âš ï¸  Staging instance not running. Checking if it exists..."
            INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=bianca-staging" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text --region ${{ env.AWS_REGION }})
            
            if [ -z "$INSTANCE_ID" ] || [ "$INSTANCE_ID" == "None" ]; then
              echo "âŒ Staging instance not found"
              exit 1
            fi
            
            echo "ğŸ”„ Starting stopped instance..."
            aws ec2 start-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "â³ Waiting for instance to start..."
            aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
            echo "âœ… Instance started, waiting for SSH service to be ready..."
            sleep 30  # Give instance time to initialize SSH service
            INSTANCE_JUST_STARTED=true
          else
            INSTANCE_JUST_STARTED=false
          fi
          
          echo "ğŸ“¡ Updating containers on instance: $INSTANCE_ID"
          
          # Get instance public IP for SSH
          echo "â³ Getting instance public IP..."
          INSTANCE_IP=$(aws ec2 describe-instances \
            --instance-ids "$INSTANCE_ID" \
            --region ${{ env.AWS_REGION }} \
            --query 'Reservations[0].Instances[0].PublicIpAddress' \
            --output text)
          
          if [ -z "$INSTANCE_IP" ] || [ "$INSTANCE_IP" == "None" ]; then
            echo "âŒ Could not get instance public IP"
            exit 1
          fi
          
          echo "ğŸ“¡ Connecting to instance: $INSTANCE_IP"
          
          # Wait for SSH to be ready (longer wait if instance was just started)
          if [ "$INSTANCE_JUST_STARTED" = "true" ]; then
            MAX_ATTEMPTS=24  # 2 minutes for newly started instances
            echo "â³ Waiting for SSH to be ready (instance was just started, allowing up to 2 minutes)..."
          else
            MAX_ATTEMPTS=12  # 1 minute for already running instances
            echo "â³ Waiting for SSH to be ready..."
          fi
          
          SSH_READY=false
          for i in $(seq 1 $MAX_ATTEMPTS); do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes -i /tmp/staging-key.pem ec2-user@$INSTANCE_IP "echo 'SSH ready'" 2>/dev/null; then
              echo "âœ… SSH is ready"
              SSH_READY=true
              break
            fi
            if [ $i -eq $MAX_ATTEMPTS ]; then
              echo "âš ï¸  SSH not ready after $((MAX_ATTEMPTS * 5)) seconds"
            else
              echo "   Waiting for SSH... ($i/$MAX_ATTEMPTS)"
              sleep 5
            fi
          done
          
          if [ "$SSH_READY" != "true" ]; then
            echo "âš ï¸  SSH connection failed. Trying SSM as fallback..."
            
            # Check if SSM is available
            SSM_READY=$(aws ssm describe-instance-information \
              --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
              --query 'InstanceInformationList[0].PingStatus' \
              --output text \
              --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Inactive")
            
            if [ "$SSM_READY" = "Online" ]; then
              echo "âœ… SSM is available, using SSM instead of SSH"
              USE_SSM=true
            else
              echo "âš ï¸  Both SSH and SSM are unavailable."
              echo "   Instance IP: $INSTANCE_IP"
              echo "   Instance ID: $INSTANCE_ID"
              echo "   SSM Status: $SSM_READY"
              echo ""
              echo "ğŸ”„ Attempting to restart the instance to fix connectivity issues..."
              
              # Check instance state
              INSTANCE_STATE=$(aws ec2 describe-instances \
                --instance-ids "$INSTANCE_ID" \
                --query 'Reservations[0].Instances[0].State.Name' \
                --output text \
                --region ${{ env.AWS_REGION }})
              
              echo "   Current instance state: $INSTANCE_STATE"
              
              if [ "$INSTANCE_STATE" = "running" ]; then
                echo "   Instance is running but not responding. Attempting reboot..."
                aws ec2 reboot-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
                echo "   â³ Waiting 60 seconds for instance to reboot..."
                sleep 60
                
                # Wait for instance to be running again
                echo "   â³ Waiting for instance to be in running state..."
                aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }} || true
                echo "   â³ Waiting additional 30 seconds for services to start..."
                sleep 30
                
                # Get new IP (might have changed)
                INSTANCE_IP=$(aws ec2 describe-instances \
                  --instance-ids "$INSTANCE_ID" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'Reservations[0].Instances[0].PublicIpAddress' \
                  --output text)
                
                echo "   New instance IP: $INSTANCE_IP"
                echo "   â³ Waiting for SSM agent to come online (up to 2 minutes)..."
                
                # Wait for SSM to come online
                SSM_WAIT_COUNT=0
                SSM_MAX_WAIT=24
                while [ $SSM_WAIT_COUNT -lt $SSM_MAX_WAIT ]; do
                  SSM_STATUS=$(aws ssm describe-instance-information \
                    --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
                    --query 'InstanceInformationList[0].PingStatus' \
                    --output text \
                    --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Inactive")
                  
                  if [ "$SSM_STATUS" = "Online" ]; then
                    echo "   âœ… SSM is now online!"
                    USE_SSM=true
                    break
                  fi
                  
                  SSM_WAIT_COUNT=$((SSM_WAIT_COUNT + 1))
                  echo "   Waiting for SSM... ($SSM_WAIT_COUNT/$SSM_MAX_WAIT) - Status: $SSM_STATUS"
                  sleep 5
                done
                
                if [ "$SSM_STATUS" != "Online" ]; then
                  echo "   âŒ SSM did not come online after reboot"
                  echo "   The instance may need manual intervention."
                  echo "   Please check the instance in the AWS console."
                  exit 1
                fi
              else
                echo "   Instance is not running (state: $INSTANCE_STATE). Starting instance..."
                aws ec2 start-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
                echo "   â³ Waiting for instance to start..."
                aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
                echo "   â³ Waiting 60 seconds for services to initialize..."
                sleep 60
                
                # Get new IP
                INSTANCE_IP=$(aws ec2 describe-instances \
                  --instance-ids "$INSTANCE_ID" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'Reservations[0].Instances[0].PublicIpAddress' \
                  --output text)
                
                echo "   Instance IP: $INSTANCE_IP"
                echo "   â³ Waiting for SSM agent to come online..."
                
                # Wait for SSM
                SSM_WAIT_COUNT=0
                SSM_MAX_WAIT=24
                while [ $SSM_WAIT_COUNT -lt $SSM_MAX_WAIT ]; do
                  SSM_STATUS=$(aws ssm describe-instance-information \
                    --filters "Key=InstanceIds,Values=$INSTANCE_ID" \
                    --query 'InstanceInformationList[0].PingStatus' \
                    --output text \
                    --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Inactive")
                  
                  if [ "$SSM_STATUS" = "Online" ]; then
                    echo "   âœ… SSM is now online!"
                    USE_SSM=true
                    break
                  fi
                  
                  SSM_WAIT_COUNT=$((SSM_WAIT_COUNT + 1))
                  echo "   Waiting for SSM... ($SSM_WAIT_COUNT/$SSM_MAX_WAIT) - Status: $SSM_STATUS"
                  sleep 5
                done
                
                if [ "$SSM_STATUS" != "Online" ]; then
                  echo "   âŒ SSM did not come online after start"
                  exit 1
                fi
              fi
            fi
          else
            USE_SSM=false
          fi
          
          if [ "$USE_SSM" = "true" ]; then
            # Use SSM to update containers
            echo "ğŸ“¡ Executing deployment via SSM..."
            set +e  # Don't exit on error - we'll check the exit code
            aws ssm send-command \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --parameters 'commands=[
                "cd /opt/bianca-staging",
                "ECR_TOKEN_FILE=/tmp/ecr-token-$(date +%Y%m%d)",
                "if [ ! -f \"$ECR_TOKEN_FILE\" ]; then timeout 30 aws ecr get-login-password --region us-east-2 | timeout 30 docker login --username AWS --password-stdin 730335291008.dkr.ecr.us-east-2.amazonaws.com && touch \"$ECR_TOKEN_FILE\"; else echo \"Using cached ECR token\"; fi",
                "echo \"ğŸ“¥ Pulling latest images (with timeout)...\"",
                "timeout 300 docker-compose pull || { echo \"âš ï¸  Pull timed out or failed, continuing...\"; }",
                "echo \"ğŸ”„ Stopping old containers...\"",
                "timeout 60 docker-compose down 2>/dev/null || true",
                "echo \"ğŸš€ Starting containers...\"",
                "timeout 120 docker-compose up -d --remove-orphans || { echo \"âŒ Failed to start containers\"; exit 1; }",
                "sleep 5",
                "docker ps --format \"table {{.Names}}\t{{.Image}}\t{{.Status}}\" | grep staging_ || true"
              ]' \
              --timeout-seconds 600 \
              --region ${{ env.AWS_REGION }} \
              --output text \
              --query 'Command.CommandId' > /tmp/ssm-command-id.txt 2>&1
            SSM_SEND_EXIT=$?
            set -e
            
            if [ $SSM_SEND_EXIT -ne 0 ] || [ ! -s /tmp/ssm-command-id.txt ]; then
              echo "âŒ SSM SendCommand failed (exit code: $SSM_SEND_EXIT)"
              cat /tmp/ssm-command-id.txt || true
              echo ""
              echo "âš ï¸  SSM SendCommand permission issue detected. Attempting instance reboot to fix..."
              
              INSTANCE_STATE=$(aws ec2 describe-instances \
                --instance-ids "$INSTANCE_ID" \
                --query 'Reservations[0].Instances[0].State.Name' \
                --output text \
                --region ${{ env.AWS_REGION }})
              
              if [ "$INSTANCE_STATE" = "running" ]; then
                echo "   Rebooting instance to refresh IAM permissions..."
                aws ec2 reboot-instances --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }}
                echo "   â³ Waiting 60 seconds for instance to reboot..."
                sleep 60
                aws ec2 wait instance-running --instance-ids "$INSTANCE_ID" --region ${{ env.AWS_REGION }} || true
                echo "   â³ Waiting additional 30 seconds for services to start..."
                sleep 30
                
                # Retry SSM SendCommand after reboot
                echo "   ğŸ”„ Retrying SSM SendCommand after reboot..."
                aws ssm send-command \
                  --instance-ids "$INSTANCE_ID" \
                  --document-name "AWS-RunShellScript" \
                  --parameters 'commands=[
                    "cd /opt/bianca-staging",
                    "ECR_TOKEN_FILE=/tmp/ecr-token-$(date +%Y%m%d)",
                    "if [ ! -f \"$ECR_TOKEN_FILE\" ]; then timeout 30 aws ecr get-login-password --region us-east-2 | timeout 30 docker login --username AWS --password-stdin 730335291008.dkr.ecr.us-east-2.amazonaws.com && touch \"$ECR_TOKEN_FILE\"; else echo \"Using cached ECR token\"; fi",
                    "echo \"ğŸ“¥ Pulling latest images (with timeout)...\"",
                    "timeout 300 docker-compose pull || { echo \"âš ï¸  Pull timed out or failed, continuing...\"; }",
                    "echo \"ğŸ”„ Stopping old containers...\"",
                    "timeout 60 docker-compose down 2>/dev/null || true",
                    "echo \"ğŸš€ Starting containers...\"",
                    "timeout 120 docker-compose up -d --remove-orphans || { echo \"âŒ Failed to start containers\"; exit 1; }",
                    "sleep 5",
                    "docker ps --format \"table {{.Names}}\t{{.Image}}\t{{.Status}}\" | grep staging_ || true"
                  ]' \
                  --timeout-seconds 600 \
                  --region ${{ env.AWS_REGION }} \
                  --output text \
                  --query 'Command.CommandId' > /tmp/ssm-command-id.txt || {
                    echo "   âŒ SSM SendCommand still failing after reboot"
                    exit 1
                  }
              else
                echo "   âŒ Instance is not running (state: $INSTANCE_STATE)"
                exit 1
              fi
            fi
            
            COMMAND_ID=$(cat /tmp/ssm-command-id.txt)
            echo "â³ Waiting for SSM command to complete (up to 10 minutes)..."
            
            # Wait for command to complete (up to 10 minutes, checking every 10 seconds)
            for i in {1..60}; do
              STATUS=$(aws ssm get-command-invocation \
                --command-id "$COMMAND_ID" \
                --instance-id "$INSTANCE_ID" \
                --query 'Status' \
                --output text \
                --region ${{ env.AWS_REGION }} 2>/dev/null || echo "Unknown")
              
              if [ "$STATUS" = "Success" ]; then
                echo "âœ… SSM command completed successfully"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardOutputContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }}
                break
              elif [ "$STATUS" = "Failed" ] || [ "$STATUS" = "Cancelled" ] || [ "$STATUS" = "TimedOut" ]; then
                echo "âŒ SSM command failed with status: $STATUS"
                echo "ğŸ“‹ Command output:"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardOutputContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }} 2>/dev/null | tail -50 || true
                echo "ğŸ“‹ Command error:"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardErrorContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }} 2>/dev/null || true
                exit 1
              fi
              
              if [ $i -eq 60 ]; then
                echo "âŒ SSM command timed out after 10 minutes"
                echo "ğŸ“‹ Partial command output:"
                aws ssm get-command-invocation \
                  --command-id "$COMMAND_ID" \
                  --instance-id "$INSTANCE_ID" \
                  --query 'StandardOutputContent' \
                  --output text \
                  --region ${{ env.AWS_REGION }} 2>/dev/null | tail -50 || true
                exit 1
              fi
              
              sleep 10
            done
          else
            # Use SSH to update containers
            echo "ğŸ”§ Executing deployment via SSH..."
            set +e  # Don't exit on error immediately - we'll check exit code manually
            ssh -o StrictHostKeyChecking=no -o ConnectTimeout=10 -i /tmp/staging-key.pem ec2-user@$INSTANCE_IP bash -c "
            set -e
            cd /opt/bianca-staging || { echo 'âŒ Failed to cd to /opt/bianca-staging'; exit 1; }
            echo 'ğŸ“¦ Logging into ECR...'
            ECR_TOKEN_FILE=/tmp/ecr-token-\$(date +%Y%m%d)
            if [ ! -f \"\$ECR_TOKEN_FILE\" ]; then
              timeout 30 aws ecr get-login-password --region us-east-2 | timeout 30 docker login --username AWS --password-stdin 730335291008.dkr.ecr.us-east-2.amazonaws.com || { echo 'âŒ ECR login failed'; exit 1; }
              touch \"\$ECR_TOKEN_FILE\"
            else
              echo '   Using cached ECR token'
            fi
            echo 'ğŸ“¥ Pulling latest images (with timeout)...'
            timeout 300 docker-compose pull || { echo 'âš ï¸  docker-compose pull had issues, but continuing...'; }
            echo ''
            echo 'ğŸ”„ Stopping and removing old containers...'
            timeout 60 docker-compose down 2>/dev/null || true
            echo 'ğŸš€ Starting containers with new images...'
            timeout 120 docker-compose up -d --remove-orphans || { echo 'âŒ Failed to start containers'; exit 1; }
            echo 'â³ Waiting for containers to start...'
            sleep 5
            echo 'ğŸ“‹ Container status:'
            docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}' | grep staging_ || echo '   No staging containers found'
            echo ''
            echo 'ğŸ” Checking image versions:'
            docker images --format 'table {{.Repository}}\t{{.Tag}}\t{{.CreatedAt}}' | grep -E 'bianca-app|730335291008' | head -5 || echo '   No images found'
            echo 'âœ… Deployment script completed'
          "
            SSH_EXIT_CODE=$?
            set -e  # Re-enable exit on error
            
            if [ $SSH_EXIT_CODE -eq 0 ]; then
              echo "âœ… Deployment completed successfully!"
            else
              echo "âŒ Deployment failed with exit code $SSH_EXIT_CODE"
              echo "   This could be due to:"
              echo "   - SSH connection issues"
              echo "   - Docker/ECR authentication problems"
              echo "   - Container startup failures"
              echo "   - Network connectivity issues"
              exit 1
            fi
          fi

